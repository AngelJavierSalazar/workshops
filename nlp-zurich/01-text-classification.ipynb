{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b909d29-9395-4313-9e9b-73628dba7b86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main ideas\n",
    "\n",
    "* Create zero-shot baseline\n",
    "* Train XLM-R on one language and zero-shot to another\n",
    "* Create learning curves to see how much labelled data we need to beat the baseline\n",
    "* Add youtube videos from the course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6a4a7-1d22-4d00-9de1-d1ec5c0a7b8a",
   "metadata": {},
   "source": [
    "ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377db4dc-ced6-4b81-b493-e4016fd99ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6218d6bc-d9ca-4ca2-b47d-ff3c239c644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HF machines\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055f734-64bb-4577-9812-47fcac4a7c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-tuning your first Transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a999088-e61b-4858-8d61-b5d32571fedb",
   "metadata": {},
   "source": [
    "Add intro. Put inference API screenshot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4bb95-d76c-44bd-96bc-307b32a1c325",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c001ff-5f00-4eb1-a60d-38edae42d3bb",
   "metadata": {},
   "source": [
    "Probably need Git LFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93553e9b-63a2-4cd7-bea3-68f0d6530988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981be30b-a9db-44fc-8d2e-818dd4255c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f22343b-a548-4f5e-ba74-30e813ea12e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98920c75-13c3-4fdd-bc1d-5fbd40a86f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this tutorial we'll use the [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi) (or MARC for short). This is a large-scale collection of Amazon product reviews in several languages: English, Japanese, German, French, Spanish, and Chinese. \n",
    "\n",
    "We can download the dataset from the Hugging Face Hub with the ðŸ¤— Datasets library, but first let's take a look at the available subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9c64f7-9657-4cc4-8b22-44258e8b013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da11c7e0cbdc416faa32541cc1703677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd8773e600e47e9be6133e985282a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['all_languages', 'de', 'en', 'es', 'fr', 'ja', 'zh']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "dataset_name = \"amazon_reviews_multi\"\n",
    "langs = get_dataset_config_names(dataset_name)\n",
    "langs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31799-31df-4072-8e6a-8bcfce64f5dc",
   "metadata": {},
   "source": [
    "Okay, we can see the language codes associated with each language, as well as an `all_languages` subset which presumably concatenates all the languages together. Let's begin by downloading the German subset with the `load_dataset()` function from ðŸ¤— Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995c653c-5cca-4427-ba31-fac74d1198c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3610c8f9b4d44a0baf4a23236a6d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "german_dataset = load_dataset(path=dataset_name, name=\"de\")\n",
    "german_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d863ee-d869-43c8-8c58-42446c344cd8",
   "metadata": {},
   "source": [
    "We can see that `german_dataset` is a `DatasetDict` object which provides a mapping between each split (`train`, `validation`, and `test`) and its corresponding `Dataset`. To access one of these splits, we need to select the key and then the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd07c0a-d36a-4df5-bde6-5c3b86357a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'de',\n",
       " 'product_category': 'sports',\n",
       " 'stars': 1,\n",
       " 'review_id': 'de_0203609',\n",
       " 'reviewer_id': 'reviewer_de_0267719',\n",
       " 'review_title': 'Leider nach 1 Jahr kaputt',\n",
       " 'review_body': 'Armband ist leider nach 1 Jahr kaputt gegangen',\n",
       " 'product_id': 'product_de_0865382'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336db506-b3b0-4d2f-8057-3f03fec2258f",
   "metadata": {},
   "source": [
    "## From Datasets to DataFrames and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0ad462-757e-4eb3-a3cf-282d74fa32a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119737</th>\n",
       "      <td>de_0970901</td>\n",
       "      <td>product_de_0712478</td>\n",
       "      <td>reviewer_de_0308094</td>\n",
       "      <td>3</td>\n",
       "      <td>Ist ok ...blondierung quillt schnell auf</td>\n",
       "      <td>Ok</td>\n",
       "      <td>de</td>\n",
       "      <td>beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72272</th>\n",
       "      <td>de_0042217</td>\n",
       "      <td>product_de_0734686</td>\n",
       "      <td>reviewer_de_0904358</td>\n",
       "      <td>2</td>\n",
       "      <td>Kein typischer Geruch oder Geschmack von einem Ghee! Ich wÃ¼rde es nicht wieder kaufen oder weiter empfehlen. Konkurrenz Produkt fand ich besser.</td>\n",
       "      <td>Kein typischer Geruch oder Geschmack von einem Ghee !</td>\n",
       "      <td>de</td>\n",
       "      <td>grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158154</th>\n",
       "      <td>de_0278932</td>\n",
       "      <td>product_de_0388890</td>\n",
       "      <td>reviewer_de_0940030</td>\n",
       "      <td>4</td>\n",
       "      <td>Dieses Buch hat mir sehr geholfen mit dem ersten Schlupf und der weiteren Aufzucht. Kann ich nur weiter empfehlen.</td>\n",
       "      <td>Sehr hilfreich</td>\n",
       "      <td>de</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65426</th>\n",
       "      <td>de_0737352</td>\n",
       "      <td>product_de_0560586</td>\n",
       "      <td>reviewer_de_0632435</td>\n",
       "      <td>2</td>\n",
       "      <td>super Schale, wunderschÃ¶n, gutes Produkt ABER Der Saugnapf geht von der Schale runter, da die MaÃŸe des Saugnapf Ringes nicht passen. Man muss aufpassen dass man den nicht dauernd neu aufsetzen muss.</td>\n",
       "      <td>der Saugnapf hÃ¤lt nicht</td>\n",
       "      <td>de</td>\n",
       "      <td>baby_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30074</th>\n",
       "      <td>de_0455430</td>\n",
       "      <td>product_de_0375951</td>\n",
       "      <td>reviewer_de_0482228</td>\n",
       "      <td>1</td>\n",
       "      <td>Artikel ist niemals angekommen, habe ihn aber bezahlt! Und dann steht noch dort ich hÃ¤tte unterschrieben, als er angeblich angekommen sei! null Sterne! Unglaublich ðŸ˜’</td>\n",
       "      <td>Artikel ist niemals angekommen!!</td>\n",
       "      <td>de</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "german_dataset.set_format(\"pandas\")\n",
    "german_df = german_dataset[\"train\"][:]\n",
    "# Create a random sample\n",
    "sample = german_df.sample(n=5, random_state=42)\n",
    "display(HTML(sample.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f464b3c1-f453-4e02-b5d8-a7d073d16e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home                        26063\n",
       "wireless                    19964\n",
       "sports                      13748\n",
       "home_improvement            12408\n",
       "apparel                     10178\n",
       "toy                          9781\n",
       "pc                           8577\n",
       "drugstore                    8075\n",
       "lawn_and_garden              7426\n",
       "beauty                       7162\n",
       "electronics                  7114\n",
       "other                        6460\n",
       "furniture                    6334\n",
       "kitchen                      5787\n",
       "automotive                   5321\n",
       "pet_products                 5028\n",
       "book                         4927\n",
       "office_product               4343\n",
       "baby_product                 4070\n",
       "shoes                        3568\n",
       "luggage                      3256\n",
       "digital_video_download       2970\n",
       "personal_care_appliances     2836\n",
       "grocery                      2737\n",
       "digital_ebook_purchase       2720\n",
       "jewelry                      2380\n",
       "camera                       1906\n",
       "watch                        1706\n",
       "video_games                  1219\n",
       "industrial_supplies          1092\n",
       "musical_instruments           844\n",
       "Name: product_category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_df[\"product_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f396c5-b0b4-47e4-b3c3-9b7002dc587d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    40000\n",
       "2    40000\n",
       "3    40000\n",
       "4    40000\n",
       "5    40000\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_df[\"stars\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68d44a1-9dce-4f80-9184-dab3c4e4a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a2e25-39c5-46b5-9f31-17c122d655b0",
   "metadata": {},
   "source": [
    "## Filtering for a domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eea2e95-e5a9-4efc-ba19-719ba81a70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_wireless(example):\n",
    "    return example[\"product_category\"] == \"wireless\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f8dfc9-8f57-43e8-abbb-51d3d28d922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-a02aa3c50d82bb3a.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-4215bd725f9f9ab7.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-6926e4ca8872dc14.arrow\n"
     ]
    }
   ],
   "source": [
    "german_dataset = german_dataset.filter(filter_for_wireless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9882852-2755-4e20-8c4e-cc027f70cd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 19964\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 491\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2420dd-b6ca-4c41-811d-e4922e400339",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Re-mapping the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e24196e-dc05-4b8f-bf30-55519305e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_dataset = german_dataset.rename_column(\"stars\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0730fcd9-f68c-44e6-8b41-932921a339c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping = {idx+1:idx for idx in range(5)}\n",
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ccc2875-f937-49c5-b483-93fa5589c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(examples):\n",
    "    return {\"labels\": label_mapping[examples[\"labels\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f4f857d-1cd0-4435-bd5e-c4cc6e072dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b900619690e246baae5ae4351dedfb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19964 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02e8a583dd14e30975d1e6ca66aa5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114edb2bce364c22a8bd4d496c8530a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/491 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'labels', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 19964\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'labels', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'labels', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 491\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dataset = german_dataset.map(map_labels)\n",
    "german_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d41cad-1a3c-4f7f-86df-8327aff415ea",
   "metadata": {},
   "source": [
    "## Creating a strong baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0996437f-0004-49e8-aff7-0aac54824c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4ecf890-4c7a-4aac-9355-4cd8af1c997c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Ich liebe dieses Buch!',\n",
       " 'labels': [3, 4, 0, 2, 1],\n",
       " 'scores': [0.2328941971063614,\n",
       "  0.2296580672264099,\n",
       "  0.21016642451286316,\n",
       "  0.19636668264865875,\n",
       "  0.13091468811035156]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_classifier(\"Ich liebe dieses Buch!\", candidate_labels=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a3718b4-bad5-4e84-a187-89789e1d5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zeroshot_preds(examples):\n",
    "    preds = zeroshot_classifier(examples[\"review_body\"], candidate_labels=[0,1,2,3,4])\n",
    "    return {\"zeroshot_prediction\": preds[\"labels\"][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66bf5aaa-10c8-49d6-8dc2-b57f169cdd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083806ed4c354d05a8ad0091a457e332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/miniconda3/envs/hf/lib/python3.9/site-packages/transformers/pipelines/base.py:899: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "german_test_dataset = german_dataset[\"test\"].map(compute_zeroshot_preds)\n",
    "german_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebadb830-788f-469e-9a4c-d6cb5d506ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 4, 0, 1, 0, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_test_dataset[\"zeroshot_prediction\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27a3fb27-96d0-417b-8695-677e81b82c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_test_dataset[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40b4e82a-ec42-41dc-8452-b8f92c74e49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2926"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(german_test_dataset[\"labels\"], german_test_dataset[\"zeroshot_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67116204-d608-4c24-aa72-9f8d169987f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2de857a8-ac62-4f0b-8376-1194718014a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "efffa20f-2983-4e86-afa3-9bf8a8d44aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(examples):\n",
    "    return tokenizer(examples[\"review_body\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0a246bc-9df9-4f80-9d68-b45a50c4ee66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8d2f00f90444aa9d370a69a06ed098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc373155ecd4ddeb95bc8706890bff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b732e369be68402989b129d49e0c3c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = german_dataset.map(tokenize_reviews, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0ef8f-95e8-4574-9a6e-4c5d9e638ca2",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9765ff12-5395-446e-ac5a-6929e12098ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aacd06-57a7-460f-82ab-372f789be822",
   "metadata": {},
   "source": [
    "## Create metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b605f30b-9fba-48dd-8d26-b96c09d1004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"MAE\": mean_absolute_error(labels, predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c48c2-e7ee-4e0f-a31c-70a23c800530",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0cddcb1-6695-4537-ab7d-be4798458b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b2d9f66-525e-4960-9860-1a8e7fb6f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "num_train_epochs = 3\n",
    "\n",
    "num_train_samples = 500\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(num_train_samples))\n",
    "logging_steps = len(train_dataset) // (batch_size * num_train_epochs)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-marc-{num_train_samples}-samples\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d45d568c-c230-41c2-b73b-a1da577c9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/git/workshops/nlp-zurich/xlm-roberta-base-finetuned-marc-500-samples is already a clone of https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc-500-samples. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5514485-36a4-436f-901d-cddf65e7383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6235263347625732,\n",
       " 'eval_MAE': 2.0,\n",
       " 'eval_runtime': 17.1764,\n",
       " 'eval_samples_per_second': 291.098,\n",
       " 'eval_steps_per_second': 18.223}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6b1f0d7-e1a1-4658-813a-18f25d2aea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 03:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.614900</td>\n",
       "      <td>1.603234</td>\n",
       "      <td>1.590600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.596700</td>\n",
       "      <td>1.589962</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.567500</td>\n",
       "      <td>1.485550</td>\n",
       "      <td>1.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.438200</td>\n",
       "      <td>1.366122</td>\n",
       "      <td>0.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.300900</td>\n",
       "      <td>1.327512</td>\n",
       "      <td>0.874600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-500-samples/checkpoint-32\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-32/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-32/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-32/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-32/special_tokens_map.json\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-500-samples/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-500-samples/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-500-samples/checkpoint-64\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-64/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-64/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-64/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-64/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-500-samples/checkpoint-96\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-96/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-96/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-500-samples/checkpoint-128\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-128/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-128/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-128/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-128/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-500-samples/checkpoint-160\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-160/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-500-samples/checkpoint-160/special_tokens_map.json\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-500-samples/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-500-samples/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=1.505726170539856, metrics={'train_runtime': 219.5709, 'train_samples_per_second': 11.386, 'train_steps_per_second': 0.729, 'total_flos': 232485434971824.0, 'train_loss': 1.505726170539856, 'epoch': 5.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48f9dae8-894c-4427-b2fd-8ac4a45d168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-500-samples\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-500-samples/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-500-samples/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-500-samples/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-500-samples/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94d7835e3d343298be1f9ebdce89cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0901c06cb4fc41718634ca9c25f7e98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct12_16-57-03_vorace/events.out.tfevents.1634050856.vorace: 100%|##########| 8.01k/8.01k [00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: error: cannot lock ref 'refs/heads/main': is at cc8d97f269f272d245b499730226a5e45c790897 but expected 1ad385c8a07abc1ca653d5ac11b1e91586cb4163        \n",
      "To https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc-500-samples\n",
      " ! [remote rejected] main -> main (failed to update ref)\n",
      "error: failed to push some refs to 'https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc-500-samples'\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "remote: error: cannot lock ref 'refs/heads/main': is at cc8d97f269f272d245b499730226a5e45c790897 but expected 1ad385c8a07abc1ca653d5ac11b1e91586cb4163        \nTo https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc-500-samples\n ! [remote rejected] main -> main (failed to update ref)\nerror: failed to push some refs to 'https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc-500-samples'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mgit_push\u001b[0;34m(self, upstream, blocking)\u001b[0m\n\u001b[1;32m    967\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m                         raise subprocess.CalledProcessError(\n\u001b[0m\u001b[1;32m    969\u001b[0m                             \u001b[0mreturn_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'push', '--set-upstream', 'origin', 'main']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3637600/729006586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   2667\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2669\u001b[0;31m         \u001b[0mgit_head_commit_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2670\u001b[0m         \u001b[0;31m# push separately the model card to be independant from the rest of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, commit_message, blocking, clean_ok)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_lfs_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_commit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         return self.git_push(\n\u001b[0m\u001b[1;32m   1195\u001b[0m             \u001b[0mupstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"origin {self.current_branch}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mgit_push\u001b[0;34m(self, upstream, blocking)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: remote: error: cannot lock ref 'refs/heads/main': is at cc8d97f269f272d245b499730226a5e45c790897 but expected 1ad385c8a07abc1ca653d5ac11b1e91586cb4163        \nTo https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc-500-samples\n ! [remote rejected] main -> main (failed to update ref)\nerror: failed to push some refs to 'https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc-500-samples'\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59472032-1c4a-400c-9eb4-324c40515c0c",
   "metadata": {},
   "source": [
    "## Zero-shot cross-lingual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77c44999-5a1d-4d34-8a72-81936158bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_corpus(lang):\n",
    "    dataset = load_dataset(dataset_name, lang, split=\"test\")\n",
    "    dataset = dataset.rename_column(\"stars\", \"labels\")\n",
    "    dataset = dataset.map(map_labels)\n",
    "    tokenized_dataset = dataset.map(tokenize_reviews, batched=True)\n",
    "    preds = trainer.evaluate(eval_dataset=tokenized_dataset)\n",
    "    return {\"MAE\": preds[\"eval_MAE\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "172ae1fb-e5a3-437d-bfc1-752837b010fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7d25a3400443de8193aab010e6ec09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03f65aa042b4114b86ac04239b80c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.874}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_corpus(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4be0db14-2abc-4597-ade1-a27196a95e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/fr/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0b09fe2ab34526b58a4124a2cc63b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544f7f29b8b34742b4e794b448245888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.8742}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_corpus(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50100e5f-2344-4baf-9a9d-acf514693651",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\", model=trainer.model, tokenizer=trainer.tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2582a5b7-0fc1-4b48-9e10-8a8b4ac76af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_3', 'score': 0.30068162083625793}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I love this book!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f80954b-5776-486a-906f-89cf9b1e1ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.372832715511322}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Ich hasse dieses Buch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19b447b9-2efd-40a0-ac4b-001b2026bf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_3', 'score': 0.2572416365146637}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"J'adore ce livre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428f746-d2c2-43b3-ac9d-d5b3a71baf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
