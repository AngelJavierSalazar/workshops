{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b909d29-9395-4313-9e9b-73628dba7b86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main ideas\n",
    "\n",
    "* Create zero-shot baseline\n",
    "* Train XLM-R on one language and zero-shot to another\n",
    "* Create learning curves to see how much labelled data we need to beat the baseline\n",
    "* Add youtube videos from the course\n",
    "* Add id2label and label2id, e.g. \"terrible\", \"poor\", \"ok\", \"good\", \"great\"\n",
    "* Add try it out\n",
    "* Add learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6a4a7-1d22-4d00-9de1-d1ec5c0a7b8a",
   "metadata": {},
   "source": [
    "ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377db4dc-ced6-4b81-b493-e4016fd99ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6218d6bc-d9ca-4ca2-b47d-ff3c239c644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HF machines\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055f734-64bb-4577-9812-47fcac4a7c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-tuning your first Transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a999088-e61b-4858-8d61-b5d32571fedb",
   "metadata": {},
   "source": [
    "In this notebook we'll take a look at fine-tuning a multilingual Transformer model called XLM-RoBERTa for text classification. At the end of this notebook you should:\n",
    "\n",
    "* Be able to load and process a dataset from the Hugging Face Hub\n",
    "* Know how to create a baseline with the zero-shot classification pipeline\n",
    "* Know how to fine-tune and evaluate pretrained model on your data\n",
    "* Know how to push a model to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4bb95-d76c-44bd-96bc-307b32a1c325",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b0663-9333-4987-a008-e1c0cd53b729",
   "metadata": {},
   "source": [
    "If you're running this notebook on Google Colab or locally, you'll need a few dependencies installed. You can install them with `pip` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454b9ec-dd54-4962-a0ea-08f85cc2352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install datasets transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c001ff-5f00-4eb1-a60d-38edae42d3bb",
   "metadata": {},
   "source": [
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93553e9b-63a2-4cd7-bea3-68f0d6530988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e532cd4-40ad-4912-afbb-d534e0591c84",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS. Uncomment and execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961c280-aa1d-464c-871d-5675f65c4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22343b-a548-4f5e-ba74-30e813ea12e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98920c75-13c3-4fdd-bc1d-5fbd40a86f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this tutorial we'll use the [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi) (or MARC for short). This is a large-scale collection of Amazon product reviews in several languages: English, Japanese, German, French, Spanish, and Chinese. \n",
    "\n",
    "We can download the dataset from the Hugging Face Hub with the ðŸ¤— Datasets library, but first let's take a look at the available subsets (also called configs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9c64f7-9657-4cc4-8b22-44258e8b013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_languages', 'de', 'en', 'es', 'fr', 'ja', 'zh']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "dataset_name = \"amazon_reviews_multi\"\n",
    "langs = get_dataset_config_names(dataset_name)\n",
    "langs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31799-31df-4072-8e6a-8bcfce64f5dc",
   "metadata": {},
   "source": [
    "Okay, we can see the language codes associated with each language, as well as an `all_languages` subset which presumably concatenates all the languages together. Let's begin by downloading the German subset with the `load_dataset()` function from ðŸ¤— Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995c653c-5cca-4427-ba31-fac74d1198c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dd2d9b6861443db23cf6d9d0731559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "marc_de = load_dataset(path=dataset_name, name=\"de\")\n",
    "marc_de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d4629-d233-4e31-a37a-e234aefe9fdc",
   "metadata": {},
   "source": [
    "One cool feature of ðŸ¤— Datasets is that `load_dataset()` will cache the files at `~/.cache/huggingface/dataset/`, so you won't need to re-download the dataset the next time your run the notebook. We can see that `german_dataset` is a `DatasetDict` object which is similar to a Python dictionary, with each key corresponding to a different split. \n",
    "\n",
    "We can access one ot these splits just like an ordinary dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d03173f-f4da-4f7b-8b10-ca0f51fe0ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "    num_rows: 200000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = marc_de[\"train\"]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3804a2c-e750-4930-8fa0-2efc6d9d4646",
   "metadata": {},
   "source": [
    "This returns a `Dataset` object which behaves like a Python container, so we can query its length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56820da3-5252-43ec-a519-b2b559fae139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5a227-2310-4dbd-9ca2-b322932b0a11",
   "metadata": {},
   "source": [
    "or access a single example by its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c37279d-c5dd-4e92-ab8a-10b184bbe6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stars': 1,\n",
       " 'product_id': 'product_de_0865382',\n",
       " 'product_category': 'sports',\n",
       " 'review_title': 'Leider nach 1 Jahr kaputt',\n",
       " 'review_id': 'de_0203609',\n",
       " 'review_body': 'Armband ist leider nach 1 Jahr kaputt gegangen',\n",
       " 'reviewer_id': 'reviewer_de_0267719',\n",
       " 'language': 'de'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25824877-c75d-420a-87c4-bcd16a380a64",
   "metadata": {},
   "source": [
    "This certainly looks like an Amazon product review (in this case the `review_body` text translates to \"Bracelet is unfortunately broken after 1 year\") and we can see the number of stars associated with the review, as well as some metadata like the language and product category. We can also see that a single row is represented as a dictionary, where the keys are the same as the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1aaa230-dd3b-405e-b596-b681cc59ba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_id',\n",
       " 'product_id',\n",
       " 'reviewer_id',\n",
       " 'stars',\n",
       " 'review_body',\n",
       " 'review_title',\n",
       " 'language',\n",
       " 'product_category']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af156a-9c3d-4593-99be-54b1a388092d",
   "metadata": {},
   "source": [
    "We can also access several rows with a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd07c0a-d36a-4df5-bde6-5c3b86357a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': ['de_0203609', 'de_0559494', 'de_0238777'],\n",
       " 'product_id': ['product_de_0865382',\n",
       "  'product_de_0678997',\n",
       "  'product_de_0372235'],\n",
       " 'reviewer_id': ['reviewer_de_0267719',\n",
       "  'reviewer_de_0783625',\n",
       "  'reviewer_de_0911426'],\n",
       " 'stars': [1, 1, 1],\n",
       " 'review_body': ['Armband ist leider nach 1 Jahr kaputt gegangen',\n",
       "  'In der Lieferung war nur Ein Akku!',\n",
       "  'Ein Stern, weil gar keine geht nicht. Es handelt sich um gebraucht Waren, die Stein haben so ein Belag drauf, wo man sich dabei denken kann, dass jemand schon die benutzt und nicht Mal richtig gewaschen. Bei ein paar ist die QualitÃ¤t Mangelhaft, siehe Bild. Ein habe ich ausprobiert, richtig gewaschen, dann verfÃ¤rbt sich..... WÃ¤rme halt nicht lange. Deswegen wird es zurÃ¼ckgeschickt.'],\n",
       " 'review_title': ['Leider nach 1 Jahr kaputt',\n",
       "  'EINS statt ZWEI Akkus!!!',\n",
       "  'Achtung Abzocke'],\n",
       " 'language': ['de', 'de', 'de'],\n",
       " 'product_category': ['sports', 'home_improvement', 'drugstore']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711979f-04fd-413a-8464-2f4e5336277e",
   "metadata": {},
   "source": [
    "and note that now we get a list of values for each column. This is because ðŸ¤— Datasets is based on Apache Arrow, which defines a typed columnar format that is very memory efficient. We can see the types that are used to represent our dataset by accessing the `features` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3972ae-a5a1-4b8e-8bcb-c1b9ccb86118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': Value(dtype='string', id=None),\n",
       " 'product_id': Value(dtype='string', id=None),\n",
       " 'reviewer_id': Value(dtype='string', id=None),\n",
       " 'stars': Value(dtype='int32', id=None),\n",
       " 'review_body': Value(dtype='string', id=None),\n",
       " 'review_title': Value(dtype='string', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'product_category': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a493c7-fd3e-475b-a4fb-caf25897753b",
   "metadata": {},
   "source": [
    "Now that we've had a quick look at the objects in ðŸ¤— Datasets, let's explore the data in more detail by using our favourite tool - Pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336db506-b3b0-4d2f-8057-3f03fec2258f",
   "metadata": {},
   "source": [
    "## From Datasets to DataFrames and back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219bc99-5b4c-4c23-937d-cfc5aedbff04",
   "metadata": {},
   "source": [
    "ðŸ¤— Datasets is designed to be interoperable with libraries like Pandas, as well as NumPy, PyTorch, TensorFlow, and JAX. To enable the conversion between various third-party libraries, ðŸ¤— Datasets provides a Dataset.set_format() function. This function only changes the output format of the dataset, so you can easily switch to another format without affecting the underlying data format which is Apache Arrow. The formatting is done in-place, so letâ€™s convert our dataset to Pandas and look at a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab052e0a-b830-4af9-a121-2c58c21569f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119737</th>\n",
       "      <td>de_0970901</td>\n",
       "      <td>product_de_0712478</td>\n",
       "      <td>reviewer_de_0308094</td>\n",
       "      <td>3</td>\n",
       "      <td>Ist ok ...blondierung quillt schnell auf</td>\n",
       "      <td>Ok</td>\n",
       "      <td>de</td>\n",
       "      <td>beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72272</th>\n",
       "      <td>de_0042217</td>\n",
       "      <td>product_de_0734686</td>\n",
       "      <td>reviewer_de_0904358</td>\n",
       "      <td>2</td>\n",
       "      <td>Kein typischer Geruch oder Geschmack von einem Ghee! Ich wÃ¼rde es nicht wieder kaufen oder weiter empfehlen. Konkurrenz Produkt fand ich besser.</td>\n",
       "      <td>Kein typischer Geruch oder Geschmack von einem Ghee !</td>\n",
       "      <td>de</td>\n",
       "      <td>grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158154</th>\n",
       "      <td>de_0278932</td>\n",
       "      <td>product_de_0388890</td>\n",
       "      <td>reviewer_de_0940030</td>\n",
       "      <td>4</td>\n",
       "      <td>Dieses Buch hat mir sehr geholfen mit dem ersten Schlupf und der weiteren Aufzucht. Kann ich nur weiter empfehlen.</td>\n",
       "      <td>Sehr hilfreich</td>\n",
       "      <td>de</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65426</th>\n",
       "      <td>de_0737352</td>\n",
       "      <td>product_de_0560586</td>\n",
       "      <td>reviewer_de_0632435</td>\n",
       "      <td>2</td>\n",
       "      <td>super Schale, wunderschÃ¶n, gutes Produkt ABER Der Saugnapf geht von der Schale runter, da die MaÃŸe des Saugnapf Ringes nicht passen. Man muss aufpassen dass man den nicht dauernd neu aufsetzen muss.</td>\n",
       "      <td>der Saugnapf hÃ¤lt nicht</td>\n",
       "      <td>de</td>\n",
       "      <td>baby_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30074</th>\n",
       "      <td>de_0455430</td>\n",
       "      <td>product_de_0375951</td>\n",
       "      <td>reviewer_de_0482228</td>\n",
       "      <td>1</td>\n",
       "      <td>Artikel ist niemals angekommen, habe ihn aber bezahlt! Und dann steht noch dort ich hÃ¤tte unterschrieben, als er angeblich angekommen sei! null Sterne! Unglaublich ðŸ˜’</td>\n",
       "      <td>Artikel ist niemals angekommen!!</td>\n",
       "      <td>de</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "marc_de.set_format(\"pandas\")\n",
    "df = marc_de[\"train\"][:]\n",
    "# Create a random sample\n",
    "sample = df.sample(n=5, random_state=42)\n",
    "display(HTML(sample.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be67d1-d588-4721-9e3c-536432fd6f27",
   "metadata": {},
   "source": [
    "We can see that the column headers are the same as we saw in the Arrow format and from the reviews we can see that negative reviews are associated with a lower star rating. Since we're now dealing with a `pandas.DataFrame` we can easily query our dataset. For example, let's see what the distribution of reviews per product category looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f464b3c1-f453-4e02-b5d8-a7d073d16e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home                        26063\n",
       "wireless                    19964\n",
       "sports                      13748\n",
       "home_improvement            12408\n",
       "apparel                     10178\n",
       "toy                          9781\n",
       "pc                           8577\n",
       "drugstore                    8075\n",
       "lawn_and_garden              7426\n",
       "beauty                       7162\n",
       "electronics                  7114\n",
       "other                        6460\n",
       "furniture                    6334\n",
       "kitchen                      5787\n",
       "automotive                   5321\n",
       "pet_products                 5028\n",
       "book                         4927\n",
       "office_product               4343\n",
       "baby_product                 4070\n",
       "shoes                        3568\n",
       "luggage                      3256\n",
       "digital_video_download       2970\n",
       "personal_care_appliances     2836\n",
       "grocery                      2737\n",
       "digital_ebook_purchase       2720\n",
       "jewelry                      2380\n",
       "camera                       1906\n",
       "watch                        1706\n",
       "video_games                  1219\n",
       "industrial_supplies          1092\n",
       "musical_instruments           844\n",
       "Name: product_category, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"product_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e36dab-8aab-4982-ab79-dfac8fd00f86",
   "metadata": {},
   "source": [
    "Okay, the `home`, `wireless`, and `sports` categories seem to be the most popular. How about the distribution of star ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f396c5-b0b4-47e4-b3c3-9b7002dc587d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    40000\n",
       "2    40000\n",
       "3    40000\n",
       "4    40000\n",
       "5    40000\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"stars\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380a769-1a11-4df5-b263-12bff67a4d00",
   "metadata": {},
   "source": [
    "In this case we can see that the dataset is balanced across each star rating, which will make it somewhat easier to evaluate our models on. Imbalanced datasets are much more common in the real-world and in these cases some additional tricks like up- or down-sampling are usually needed.\n",
    "\n",
    "Now that we've got a rough idea about the kind of data we're dealing with, let's reset the output format from `pandas` back to `arrow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d68d44a1-9dce-4f80-9184-dab3c4e4a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "marc_de.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a2e25-39c5-46b5-9f31-17c122d655b0",
   "metadata": {},
   "source": [
    "## Filtering for a product category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6a094-55a9-4c41-ae2f-e5b662f21e6d",
   "metadata": {},
   "source": [
    "Although we could go ahead and fine-tune a Transformer model on the whole set of 200,000 German reviews, this will take several hours on a single GPU. So instead, we'll focus on fine-tuning a model for a single product category! In ðŸ¤— Datasets, we can filter data very quickly by using the `Dataset.filter()` method. This method expects a function that returns Boolean values, in our case `True` if the `product_category` matches the chosen category and `False` otherwise. Here's one way to implement this, and we'll pick the `sports` category as the domain to train on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eea2e95-e5a9-4efc-ba19-719ba81a70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_category = \"sports\"\n",
    "\n",
    "def filter_for_product(example, product_category=product_category):\n",
    "    return example[\"product_category\"] == product_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d848f-f3f8-43ea-a12c-90d49dbb27f6",
   "metadata": {},
   "source": [
    "Now when we pass `filter_for_product()` to `Dataset.filter()` we get a filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a2aecb4-6501-487d-a38e-5fbe3681c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-415aaa5af094e2c4.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-60d769fb1e7b4f2f.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-9618290228399c16.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 13748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 339\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 329\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dataset = marc_de.filter(filter_for_product)\n",
    "product_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa86ae7-6789-44e3-b1f8-a88b9383aec9",
   "metadata": {},
   "source": [
    "Yep, this looks good - we have 13,748 reviews in the train split which agrees the number we saw in the distribution of categories earlier. Let's do a quick sanity check by taking a look at a few samples. Here ðŸ¤— Datasets provides `Dataset.shuffle()` and `Dataset.select()` functions that we can chain to get a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d74a059-6381-45ce-bc9d-736ed3bac0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-34cfe66dac005389.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review_id': ['de_0592068', 'de_0659764', 'de_0617399'],\n",
       " 'product_id': ['product_de_0646545',\n",
       "  'product_de_0628607',\n",
       "  'product_de_0596523'],\n",
       " 'reviewer_id': ['reviewer_de_0065351',\n",
       "  'reviewer_de_0938057',\n",
       "  'reviewer_de_0996678'],\n",
       " 'stars': [5, 2, 3],\n",
       " 'review_body': ['Dieses aufblasbare Sofa ist sehr einfach aufzubauen (einfach in Wind halten) und leicht wieder einzupacken. Es war gut verpackt (eine Tasche mit Tragegurt war dabei), hat AufbewahrungsmÃ¶glichkeiten an der Rechten Seite und einen Hering zum Befestigen am oberen Rand. Es ist sehr bequem und die Preis/Leistung ist einfach super! Ich kann es wirklich nur empfehlen :)',\n",
       "  'Leider nach ca. 1 Jahr ist die Schnalle abgerissen. Schade!!!',\n",
       "  'An sich ist das X-Bike nicht schlecht bis auf die Verarbeitung vom Computer. Sehr zu bemÃ¤ngeln habe ich aber die Pedalen bzw. Die Kugellager darin, am zweiten Tag und nach circa 30 km haben sich die Kugellager aufgelÃ¶st. Pedale lassen dich nicht mehr drehen.'],\n",
       " 'review_title': ['Sehr bequem und top Preis/Leistung!',\n",
       "  'Super Produkt, aber leider nach ca. 1 Jahr ist die Schnalle abgerissen. Schade!!!',\n",
       "  'X-Bike Pedal Kugellager sehr schlechte QualitÃ¤t'],\n",
       " 'language': ['de', 'de', 'de'],\n",
       " 'product_category': ['sports', 'sports', 'sports']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dataset[\"train\"].shuffle(seed=42).select(range(3))[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ddc84-b48d-4853-8abe-fac0460d8dbe",
   "metadata": {},
   "source": [
    "Okay, now that we have our corpus of sports reviews, let's do one last bit of data preparation: creating label mappings from star ratings to human readable strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2420dd-b6ca-4c41-811d-e4922e400339",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Re-mapping the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642ae4c-5c7f-42d6-acc3-4f099b12e63a",
   "metadata": {},
   "source": [
    "During training, ðŸ¤— Transformers expects the labels to be ordered, starting from 0 to N. But we've seen that our star ratings range from 1-5, so let's fix that. While we're at it, we'll create a mapping between the label IDs and names, which will be handy later on when we want to run inference with our model. First we'll define the label mapping from ID to name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3367d29d-f52d-4500-8aca-d8182541498c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'terrible', 1: 'poor', 2: 'ok', 3: 'good', 4: 'great'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"terrible\", \"poor\", \"ok\", \"good\", \"great\"]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495a896-5960-49bd-a09f-47954b843e7b",
   "metadata": {},
   "source": [
    "We can then apply this mapping to our whole dataset by using the `Dataset.map()` method. Similar to the `Dataset.filter()` method, this one expects a function which receives examples as input, but returns a Python dictionary as output. The keys of the dictionary correspond to the columns, while the values correspond to the column entries. The following function creates two new columns:\n",
    "\n",
    "* A `labels` column which is the star rating shifted down by one\n",
    "* A `label_name` column which provides a nice string for each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb136d26-85c8-4e54-ba42-a8e8c6339364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(example):\n",
    "    # Shift labels to start from 0\n",
    "    label_id = example[\"stars\"] - 1\n",
    "    return {\"labels\": label_id, \"label_name\": id2label[label_id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e600b8-c2d4-43b2-8c7e-4fc2690c3307",
   "metadata": {},
   "source": [
    "To apply this mapping, we simply feed it to `Dataset.map` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a29e6267-94ec-471f-bb94-e1a1ab1b4c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ced5c0e84a404184063234d34eee7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13748 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52241a119dd64f76b3710a2f51d24146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019da5aa52fb470eb766a7de633e2974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'stars': 1,\n",
       " 'product_id': 'product_de_0865382',\n",
       " 'product_category': 'sports',\n",
       " 'review_title': 'Leider nach 1 Jahr kaputt',\n",
       " 'label_name': 'terrible',\n",
       " 'labels': 0,\n",
       " 'review_id': 'de_0203609',\n",
       " 'review_body': 'Armband ist leider nach 1 Jahr kaputt gegangen',\n",
       " 'reviewer_id': 'reviewer_de_0267719',\n",
       " 'language': 'de'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dataset = product_dataset.map(map_labels)\n",
    "# Peek at the first example\n",
    "product_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43aa90f-71df-4c46-b724-42f1643e2bdc",
   "metadata": {},
   "source": [
    "Great, it works! We'll also need the reverse label mapping later, so let's define it here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d7b295a-32fb-461a-a246-bf68fe4da147",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d41cad-1a3c-4f7f-86df-8327aff415ea",
   "metadata": {},
   "source": [
    "## Creating a strong baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0996437f-0004-49e8-aff7-0aac54824c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4ecf890-4c7a-4aac-9355-4cd8af1c997c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Dieser Wifi-Router ist perfect!',\n",
       " 'labels': ['great', 'good', 'ok', 'poor', 'terrible'],\n",
       " 'scores': [0.4981793165206909,\n",
       "  0.3877113461494446,\n",
       "  0.1132708266377449,\n",
       "  0.0005077120731584728,\n",
       "  0.00033083048765547574]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_classifier(\"Dieser Wifi-Router ist perfect!\", candidate_labels=[\"terrible\",\"poor\",\"ok\",\"good\",\"great\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a3718b4-bad5-4e84-a187-89789e1d5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zeroshot_preds(examples):\n",
    "    preds = zeroshot_classifier(examples[\"review_body\"], candidate_labels=[0,1,2,3,4])\n",
    "    return {\"zeroshot_prediction\": preds[\"labels\"][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66bf5aaa-10c8-49d6-8dc2-b57f169cdd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f37953ff54407b930cb8de0564cdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/491 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/miniconda3/envs/hf/lib/python3.9/site-packages/transformers/pipelines/base.py:899: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review_id', 'product_id', 'reviewer_id', 'labels', 'review_body', 'review_title', 'language', 'product_category', 'zeroshot_prediction'],\n",
       "    num_rows: 491\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wireless_test_dataset = wireless_dataset[\"test\"].map(compute_zeroshot_preds)\n",
    "wireless_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebadb830-788f-469e-9a4c-d6cb5d506ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 1, 0, 0, 0, 1, 2, 1, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wireless_test_dataset[\"zeroshot_prediction\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27a3fb27-96d0-417b-8695-677e81b82c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wireless_test_dataset[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40b4e82a-ec42-41dc-8452-b8f92c74e49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3217922606924644"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(wireless_test_dataset[\"labels\"], wireless_test_dataset[\"zeroshot_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67116204-d608-4c24-aa72-9f8d169987f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## From text to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2de857a8-ac62-4f0b-8376-1194718014a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efffa20f-2983-4e86-afa3-9bf8a8d44aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(examples):\n",
    "    return tokenizer(examples[\"review_body\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0a246bc-9df9-4f80-9d68-b45a50c4ee66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf2422097ca483abb8e3dc8d18cca9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b12b1372cd34e4bba3a8361302709d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15650a6eba2461cbecf030c5e62aeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'language', 'product_category', 'product_id', 'review_body', 'review_id', 'review_title', 'reviewer_id'],\n",
       "        num_rows: 19964\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'language', 'product_category', 'product_id', 'review_body', 'review_id', 'review_title', 'reviewer_id'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'language', 'product_category', 'product_id', 'review_body', 'review_id', 'review_title', 'reviewer_id'],\n",
       "        num_rows: 491\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = wireless_dataset.map(tokenize_reviews, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0ef8f-95e8-4574-9a6e-4c5d9e638ca2",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9765ff12-5395-446e-ac5a-6929e12098ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aacd06-57a7-460f-82ab-372f789be822",
   "metadata": {},
   "source": [
    "## Create metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b605f30b-9fba-48dd-8d26-b96c09d1004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"MAE\": mean_absolute_error(labels, predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c48c2-e7ee-4e0f-a31c-70a23c800530",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0cddcb1-6695-4537-ab7d-be4798458b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b2d9f66-525e-4960-9860-1a8e7fb6f149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-d522bf90a337de81.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "num_train_epochs = 2\n",
    "logging_steps = len(tokenized_dataset[\"train\"]) // (batch_size * num_train_epochs)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-marc\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d45d568c-c230-41c2-b73b-a1da577c9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc-19964-samples into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer \n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5514485-36a4-436f-901d-cddf65e7383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: review_body, review_id, reviewer_id, product_id, product_category, language, review_title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6095925569534302,\n",
       " 'eval_MAE': 2.104,\n",
       " 'eval_runtime': 1.6249,\n",
       " 'eval_samples_per_second': 307.72,\n",
       " 'eval_steps_per_second': 19.694}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6b1f0d7-e1a1-4658-813a-18f25d2aea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: review_body, review_id, reviewer_id, product_id, product_category, language, review_title.\n",
      "***** Running training *****\n",
      "  Num examples = 19964\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2496\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2496' max='2496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2496/2496 10:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.017000</td>\n",
       "      <td>0.948932</td>\n",
       "      <td>0.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.890300</td>\n",
       "      <td>0.947390</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: review_body, review_id, reviewer_id, product_id, product_category, language, review_title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-1248\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-1248/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-1248/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-1248/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-1248/special_tokens_map.json\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-19964-samples/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-19964-samples/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: review_body, review_id, reviewer_id, product_id, product_category, language, review_title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-2496\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-2496/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-2496/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-2496/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-19964-samples/checkpoint-2496/special_tokens_map.json\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-19964-samples/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-19964-samples/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2496, training_loss=1.0097694710279121, metrics={'train_runtime': 626.4529, 'train_samples_per_second': 63.737, 'train_steps_per_second': 3.984, 'total_flos': 3456130803242208.0, 'train_loss': 1.0097694710279121, 'epoch': 2.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48f9dae8-894c-4427-b2fd-8ac4a45d168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc-19964-samples\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc-19964-samples/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc-19964-samples/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc-19964-samples/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc-19964-samples/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "On branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mgit_commit\u001b[0;34m(self, commit_message)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             result = subprocess.run(\n\u001b[0m\u001b[1;32m    899\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m\"git\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"commit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-v\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    529\u001b[0m                                      output=stdout, stderr=stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'commit', '-m', 'Training complete', '-v']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4082883/869197989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   2667\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2669\u001b[0;31m         \u001b[0mgit_head_commit_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2670\u001b[0m         \u001b[0;31m# push separately the model card to be independant from the rest of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, commit_message, blocking, clean_ok)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_lfs_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_commit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m         return self.git_push(\n\u001b[1;32m   1195\u001b[0m             \u001b[0mupstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"origin {self.current_branch}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hf/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mgit_commit\u001b[0;34m(self, commit_message)\u001b[0m\n\u001b[1;32m    909\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     def git_push(\n",
      "\u001b[0;31mOSError\u001b[0m: On branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\", blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59472032-1c4a-400c-9eb4-324c40515c0c",
   "metadata": {},
   "source": [
    "## Zero-shot cross-lingual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77c44999-5a1d-4d34-8a72-81936158bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_corpus(lang):\n",
    "    dataset = load_dataset(dataset_name, lang, split=\"test\")\n",
    "    dataset = dataset.rename_column(\"stars\", \"labels\")\n",
    "    dataset = dataset.map(map_labels)\n",
    "    tokenized_dataset = dataset.map(tokenize_reviews, batched=True)\n",
    "    preds = trainer.evaluate(eval_dataset=tokenized_dataset)\n",
    "    return {\"MAE\": preds[\"eval_MAE\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "172ae1fb-e5a3-437d-bfc1-752837b010fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7d25a3400443de8193aab010e6ec09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03f65aa042b4114b86ac04239b80c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.874}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_corpus(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4be0db14-2abc-4597-ade1-a27196a95e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/fr/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0b09fe2ab34526b58a4124a2cc63b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544f7f29b8b34742b4e794b448245888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: reviewer_id, product_id, review_title, review_body, review_id, product_category, language.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.8742}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_corpus(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af782a3-6b3c-4597-8490-d9ca5ce37933",
   "metadata": {},
   "source": [
    "## Using your fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50100e5f-2344-4baf-9a9d-acf514693651",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\", model=trainer.model, tokenizer=trainer.tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2582a5b7-0fc1-4b48-9e10-8a8b4ac76af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_3', 'score': 0.30068162083625793}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I love this book!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f80954b-5776-486a-906f-89cf9b1e1ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.372832715511322}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Ich hasse dieses Buch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19b447b9-2efd-40a0-ac4b-001b2026bf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_3', 'score': 0.2572416365146637}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"J'adore ce livre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428f746-d2c2-43b3-ac9d-d5b3a71baf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
