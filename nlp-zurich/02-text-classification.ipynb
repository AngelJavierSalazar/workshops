{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377db4dc-ced6-4b81-b493-e4016fd99ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6218d6bc-d9ca-4ca2-b47d-ff3c239c644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HF machines\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055f734-64bb-4577-9812-47fcac4a7c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-tuning your first Transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a999088-e61b-4858-8d61-b5d32571fedb",
   "metadata": {},
   "source": [
    "In this notebook we'll take a look at fine-tuning a multilingual Transformer model called [XLM-RoBERTa](https://huggingface.co/xlm-roberta-base) for text classification. By the end of this notebook you should know how to:\n",
    "\n",
    "* Load and process a dataset from the Hugging Face Hub\n",
    "* Create a baseline with the zero-shot classification pipeline\n",
    "* Fine-tune and evaluate pretrained model on your data\n",
    "* Push a model to the Hugging Face Hub\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4bb95-d76c-44bd-96bc-307b32a1c325",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b0663-9333-4987-a008-e1c0cd53b729",
   "metadata": {},
   "source": [
    "If you're running this notebook on Google Colab or locally, you'll need a few dependencies installed. You can install them with `pip` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454b9ec-dd54-4962-a0ea-08f85cc2352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install datasets transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c001ff-5f00-4eb1-a60d-38edae42d3bb",
   "metadata": {},
   "source": [
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93553e9b-63a2-4cd7-bea3-68f0d6530988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e532cd4-40ad-4912-afbb-d534e0591c84",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS. Uncomment and execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961c280-aa1d-464c-871d-5675f65c4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22343b-a548-4f5e-ba74-30e813ea12e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98920c75-13c3-4fdd-bc1d-5fbd40a86f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this tutorial we'll use the [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi) (or MARC for short). This is a large-scale collection of Amazon product reviews in several languages: English, Japanese, German, French, Spanish, and Chinese. \n",
    "\n",
    "We can download the dataset from the Hugging Face Hub with the ðŸ¤— Datasets library, but first let's take a look at the available subsets (also called configs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9c64f7-9657-4cc4-8b22-44258e8b013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_languages', 'de', 'en', 'es', 'fr', 'ja', 'zh']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "dataset_name = \"amazon_reviews_multi\"\n",
    "langs = get_dataset_config_names(dataset_name)\n",
    "langs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31799-31df-4072-8e6a-8bcfce64f5dc",
   "metadata": {},
   "source": [
    "Okay, we can see the language codes associated with each language, as well as an `all_languages` subset which presumably concatenates all the languages together. Let's begin by downloading the German subset with the `load_dataset()` function from ðŸ¤— Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995c653c-5cca-4427-ba31-fac74d1198c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae57f60260d424ea923215c1a1ec1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "marc_de = load_dataset(path=dataset_name, name=\"de\")\n",
    "marc_de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d4629-d233-4e31-a37a-e234aefe9fdc",
   "metadata": {},
   "source": [
    "One cool feature of ðŸ¤— Datasets is that `load_dataset()` will cache the files at `~/.cache/huggingface/dataset/`, so you won't need to re-download the dataset the next time your run the notebook. We can see that `german_dataset` is a `DatasetDict` object which is similar to a Python dictionary, with each key corresponding to a different split. \n",
    "\n",
    "We can access one ot these splits just like an ordinary dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d03173f-f4da-4f7b-8b10-ca0f51fe0ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "    num_rows: 200000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = marc_de[\"train\"]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3804a2c-e750-4930-8fa0-2efc6d9d4646",
   "metadata": {},
   "source": [
    "This returns a `Dataset` object which behaves like a Python container, so we can query its length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56820da3-5252-43ec-a519-b2b559fae139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5a227-2310-4dbd-9ca2-b322932b0a11",
   "metadata": {},
   "source": [
    "or access a single example by its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c37279d-c5dd-4e92-ab8a-10b184bbe6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'de',\n",
       " 'product_category': 'sports',\n",
       " 'review_body': 'Armband ist leider nach 1 Jahr kaputt gegangen',\n",
       " 'review_id': 'de_0203609',\n",
       " 'stars': 1,\n",
       " 'reviewer_id': 'reviewer_de_0267719',\n",
       " 'product_id': 'product_de_0865382',\n",
       " 'review_title': 'Leider nach 1 Jahr kaputt'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25824877-c75d-420a-87c4-bcd16a380a64",
   "metadata": {},
   "source": [
    "This certainly looks like an Amazon product review (in this case the `review_body` text translates to \"Bracelet is unfortunately broken after 1 year\") and we can see the number of stars associated with the review, as well as some metadata like the language and product category. We can also see that a single row is represented as a dictionary, where the keys are the same as the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1aaa230-dd3b-405e-b596-b681cc59ba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_id',\n",
       " 'product_id',\n",
       " 'reviewer_id',\n",
       " 'stars',\n",
       " 'review_body',\n",
       " 'review_title',\n",
       " 'language',\n",
       " 'product_category']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af156a-9c3d-4593-99be-54b1a388092d",
   "metadata": {},
   "source": [
    "We can also access several rows with a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd07c0a-d36a-4df5-bde6-5c3b86357a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': ['de_0203609', 'de_0559494', 'de_0238777'],\n",
       " 'product_id': ['product_de_0865382',\n",
       "  'product_de_0678997',\n",
       "  'product_de_0372235'],\n",
       " 'reviewer_id': ['reviewer_de_0267719',\n",
       "  'reviewer_de_0783625',\n",
       "  'reviewer_de_0911426'],\n",
       " 'stars': [1, 1, 1],\n",
       " 'review_body': ['Armband ist leider nach 1 Jahr kaputt gegangen',\n",
       "  'In der Lieferung war nur Ein Akku!',\n",
       "  'Ein Stern, weil gar keine geht nicht. Es handelt sich um gebraucht Waren, die Stein haben so ein Belag drauf, wo man sich dabei denken kann, dass jemand schon die benutzt und nicht Mal richtig gewaschen. Bei ein paar ist die QualitÃ¤t Mangelhaft, siehe Bild. Ein habe ich ausprobiert, richtig gewaschen, dann verfÃ¤rbt sich..... WÃ¤rme halt nicht lange. Deswegen wird es zurÃ¼ckgeschickt.'],\n",
       " 'review_title': ['Leider nach 1 Jahr kaputt',\n",
       "  'EINS statt ZWEI Akkus!!!',\n",
       "  'Achtung Abzocke'],\n",
       " 'language': ['de', 'de', 'de'],\n",
       " 'product_category': ['sports', 'home_improvement', 'drugstore']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711979f-04fd-413a-8464-2f4e5336277e",
   "metadata": {},
   "source": [
    "and note that now we get a list of values for each column. This is because ðŸ¤— Datasets is based on Apache Arrow, which defines a typed columnar format that is very memory efficient. We can see the types that are used to represent our dataset by accessing the `features` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a3972ae-a5a1-4b8e-8bcb-c1b9ccb86118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': Value(dtype='string', id=None),\n",
       " 'product_id': Value(dtype='string', id=None),\n",
       " 'reviewer_id': Value(dtype='string', id=None),\n",
       " 'stars': Value(dtype='int32', id=None),\n",
       " 'review_body': Value(dtype='string', id=None),\n",
       " 'review_title': Value(dtype='string', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'product_category': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a493c7-fd3e-475b-a4fb-caf25897753b",
   "metadata": {},
   "source": [
    "Now that we've had a quick look at the objects in ðŸ¤— Datasets, let's explore the data in more detail by using our favourite tool - Pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336db506-b3b0-4d2f-8057-3f03fec2258f",
   "metadata": {},
   "source": [
    "## From Datasets to DataFrames and back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219bc99-5b4c-4c23-937d-cfc5aedbff04",
   "metadata": {},
   "source": [
    "ðŸ¤— Datasets is designed to be interoperable with libraries like Pandas, as well as NumPy, PyTorch, TensorFlow, and JAX. To enable the conversion between various third-party libraries, ðŸ¤— Datasets provides a Dataset.set_format() function. This function only changes the output format of the dataset, so you can easily switch to another format without affecting the underlying data format which is Apache Arrow. The formatting is done in-place, so letâ€™s convert our dataset to Pandas and look at a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab052e0a-b830-4af9-a121-2c58c21569f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119737</th>\n",
       "      <td>de_0970901</td>\n",
       "      <td>product_de_0712478</td>\n",
       "      <td>reviewer_de_0308094</td>\n",
       "      <td>3</td>\n",
       "      <td>Ist ok ...blondierung quillt schnell auf</td>\n",
       "      <td>Ok</td>\n",
       "      <td>de</td>\n",
       "      <td>beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72272</th>\n",
       "      <td>de_0042217</td>\n",
       "      <td>product_de_0734686</td>\n",
       "      <td>reviewer_de_0904358</td>\n",
       "      <td>2</td>\n",
       "      <td>Kein typischer Geruch oder Geschmack von einem Ghee! Ich wÃ¼rde es nicht wieder kaufen oder weiter empfehlen. Konkurrenz Produkt fand ich besser.</td>\n",
       "      <td>Kein typischer Geruch oder Geschmack von einem Ghee !</td>\n",
       "      <td>de</td>\n",
       "      <td>grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158154</th>\n",
       "      <td>de_0278932</td>\n",
       "      <td>product_de_0388890</td>\n",
       "      <td>reviewer_de_0940030</td>\n",
       "      <td>4</td>\n",
       "      <td>Dieses Buch hat mir sehr geholfen mit dem ersten Schlupf und der weiteren Aufzucht. Kann ich nur weiter empfehlen.</td>\n",
       "      <td>Sehr hilfreich</td>\n",
       "      <td>de</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65426</th>\n",
       "      <td>de_0737352</td>\n",
       "      <td>product_de_0560586</td>\n",
       "      <td>reviewer_de_0632435</td>\n",
       "      <td>2</td>\n",
       "      <td>super Schale, wunderschÃ¶n, gutes Produkt ABER Der Saugnapf geht von der Schale runter, da die MaÃŸe des Saugnapf Ringes nicht passen. Man muss aufpassen dass man den nicht dauernd neu aufsetzen muss.</td>\n",
       "      <td>der Saugnapf hÃ¤lt nicht</td>\n",
       "      <td>de</td>\n",
       "      <td>baby_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30074</th>\n",
       "      <td>de_0455430</td>\n",
       "      <td>product_de_0375951</td>\n",
       "      <td>reviewer_de_0482228</td>\n",
       "      <td>1</td>\n",
       "      <td>Artikel ist niemals angekommen, habe ihn aber bezahlt! Und dann steht noch dort ich hÃ¤tte unterschrieben, als er angeblich angekommen sei! null Sterne! Unglaublich ðŸ˜’</td>\n",
       "      <td>Artikel ist niemals angekommen!!</td>\n",
       "      <td>de</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "marc_de.set_format(\"pandas\")\n",
    "df = marc_de[\"train\"][:]\n",
    "# Create a random sample\n",
    "sample = df.sample(n=5, random_state=42)\n",
    "display(HTML(sample.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be67d1-d588-4721-9e3c-536432fd6f27",
   "metadata": {},
   "source": [
    "We can see that the column headers are the same as we saw in the Arrow format and from the reviews we can see that negative reviews are associated with a lower star rating. Since we're now dealing with a `pandas.DataFrame` we can easily query our dataset. For example, let's see what the distribution of reviews per product category looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f464b3c1-f453-4e02-b5d8-a7d073d16e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home                        26063\n",
       "wireless                    19964\n",
       "sports                      13748\n",
       "home_improvement            12408\n",
       "apparel                     10178\n",
       "toy                          9781\n",
       "pc                           8577\n",
       "drugstore                    8075\n",
       "lawn_and_garden              7426\n",
       "beauty                       7162\n",
       "electronics                  7114\n",
       "other                        6460\n",
       "furniture                    6334\n",
       "kitchen                      5787\n",
       "automotive                   5321\n",
       "pet_products                 5028\n",
       "book                         4927\n",
       "office_product               4343\n",
       "baby_product                 4070\n",
       "shoes                        3568\n",
       "luggage                      3256\n",
       "digital_video_download       2970\n",
       "personal_care_appliances     2836\n",
       "grocery                      2737\n",
       "digital_ebook_purchase       2720\n",
       "jewelry                      2380\n",
       "camera                       1906\n",
       "watch                        1706\n",
       "video_games                  1219\n",
       "industrial_supplies          1092\n",
       "musical_instruments           844\n",
       "Name: product_category, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"product_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e36dab-8aab-4982-ab79-dfac8fd00f86",
   "metadata": {},
   "source": [
    "Okay, the `home`, `wireless`, and `sports` categories seem to be the most popular. How about the distribution of star ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56f396c5-b0b4-47e4-b3c3-9b7002dc587d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    40000\n",
       "2    40000\n",
       "3    40000\n",
       "4    40000\n",
       "5    40000\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"stars\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380a769-1a11-4df5-b263-12bff67a4d00",
   "metadata": {},
   "source": [
    "In this case we can see that the dataset is balanced across each star rating, which will make it somewhat easier to evaluate our models on. Imbalanced datasets are much more common in the real-world and in these cases some additional tricks like up- or down-sampling are usually needed.\n",
    "\n",
    "Now that we've got a rough idea about the kind of data we're dealing with, let's reset the output format from `pandas` back to `arrow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d68d44a1-9dce-4f80-9184-dab3c4e4a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "marc_de.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a2e25-39c5-46b5-9f31-17c122d655b0",
   "metadata": {},
   "source": [
    "## Filtering for a product category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6a094-55a9-4c41-ae2f-e5b662f21e6d",
   "metadata": {},
   "source": [
    "Although we could go ahead and fine-tune a Transformer model on the whole set of 200,000 German reviews, this will take several hours on a single GPU. So instead, we'll focus on fine-tuning a model for a single product category! In ðŸ¤— Datasets, we can filter data very quickly by using the `Dataset.filter()` method. This method expects a function that returns Boolean values, in our case `True` if the `product_category` matches the chosen category and `False` otherwise. Here's one way to implement this, and we'll pick the `sports` category as the domain to train on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eea2e95-e5a9-4efc-ba19-719ba81a70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_category = \"sports\"\n",
    "\n",
    "def filter_for_product(example, product_category=product_category):\n",
    "    return example[\"product_category\"] == product_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d848f-f3f8-43ea-a12c-90d49dbb27f6",
   "metadata": {},
   "source": [
    "Now when we pass `filter_for_product()` to `Dataset.filter()` we get a filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2aecb4-6501-487d-a38e-5fbe3681c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-415aaa5af094e2c4.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-60d769fb1e7b4f2f.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-9618290228399c16.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 13748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 339\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 329\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dataset = marc_de.filter(filter_for_product)\n",
    "product_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa86ae7-6789-44e3-b1f8-a88b9383aec9",
   "metadata": {},
   "source": [
    "Yep, this looks good - we have 13,748 reviews in the train split which agrees the number we saw in the distribution of categories earlier. Let's do a quick sanity check by taking a look at a few samples. Here ðŸ¤— Datasets provides `Dataset.shuffle()` and `Dataset.select()` functions that we can chain to get a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d74a059-6381-45ce-bc9d-736ed3bac0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-34cfe66dac005389.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review_id': ['de_0592068', 'de_0659764', 'de_0617399'],\n",
       " 'product_id': ['product_de_0646545',\n",
       "  'product_de_0628607',\n",
       "  'product_de_0596523'],\n",
       " 'reviewer_id': ['reviewer_de_0065351',\n",
       "  'reviewer_de_0938057',\n",
       "  'reviewer_de_0996678'],\n",
       " 'stars': [5, 2, 3],\n",
       " 'review_body': ['Dieses aufblasbare Sofa ist sehr einfach aufzubauen (einfach in Wind halten) und leicht wieder einzupacken. Es war gut verpackt (eine Tasche mit Tragegurt war dabei), hat AufbewahrungsmÃ¶glichkeiten an der Rechten Seite und einen Hering zum Befestigen am oberen Rand. Es ist sehr bequem und die Preis/Leistung ist einfach super! Ich kann es wirklich nur empfehlen :)',\n",
       "  'Leider nach ca. 1 Jahr ist die Schnalle abgerissen. Schade!!!',\n",
       "  'An sich ist das X-Bike nicht schlecht bis auf die Verarbeitung vom Computer. Sehr zu bemÃ¤ngeln habe ich aber die Pedalen bzw. Die Kugellager darin, am zweiten Tag und nach circa 30 km haben sich die Kugellager aufgelÃ¶st. Pedale lassen dich nicht mehr drehen.'],\n",
       " 'review_title': ['Sehr bequem und top Preis/Leistung!',\n",
       "  'Super Produkt, aber leider nach ca. 1 Jahr ist die Schnalle abgerissen. Schade!!!',\n",
       "  'X-Bike Pedal Kugellager sehr schlechte QualitÃ¤t'],\n",
       " 'language': ['de', 'de', 'de'],\n",
       " 'product_category': ['sports', 'sports', 'sports']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dataset[\"train\"].shuffle(seed=42).select(range(3))[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ddc84-b48d-4853-8abe-fac0460d8dbe",
   "metadata": {},
   "source": [
    "Okay, now that we have our corpus of sports reviews, let's do one last bit of data preparation: creating label mappings from star ratings to human readable strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2420dd-b6ca-4c41-811d-e4922e400339",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mapping the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642ae4c-5c7f-42d6-acc3-4f099b12e63a",
   "metadata": {},
   "source": [
    "During training, ðŸ¤— Transformers expects the labels to be ordered, starting from 0 to N. But we've seen that our star ratings range from 1-5, so let's fix that. While we're at it, we'll create a mapping between the label IDs and names, which will be handy later on when we want to run inference with our model. First we'll define the label mapping from ID to name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3367d29d-f52d-4500-8aca-d8182541498c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'terrible', 1: 'poor', 2: 'ok', 3: 'good', 4: 'great'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = [\"terrible\", \"poor\", \"ok\", \"good\", \"great\"]\n",
    "id2label = {idx:label for idx, label in enumerate(label_names)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495a896-5960-49bd-a09f-47954b843e7b",
   "metadata": {},
   "source": [
    "We can then apply this mapping to our whole dataset by using the `Dataset.map()` method. Similar to the `Dataset.filter()` method, this one expects a function which receives examples as input, but returns a Python dictionary as output. The keys of the dictionary correspond to the columns, while the values correspond to the column entries. The following function creates two new columns:\n",
    "\n",
    "* A `labels` column which is the star rating shifted down by one\n",
    "* A `label_name` column which provides a nice string for each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb136d26-85c8-4e54-ba42-a8e8c6339364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(example):\n",
    "    # Shift labels to start from 0\n",
    "    label_id = example[\"stars\"] - 1\n",
    "    return {\"labels\": label_id, \"label_name\": id2label[label_id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e600b8-c2d4-43b2-8c7e-4fc2690c3307",
   "metadata": {},
   "source": [
    "To apply this mapping, we simply feed it to `Dataset.map` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29e6267-94ec-471f-bb94-e1a1ab1b4c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-3305e19e1599db7f.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-ff534f940f83173d.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-9cc9183bef8d8635.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'language': 'de',\n",
       " 'label_name': 'terrible',\n",
       " 'product_category': 'sports',\n",
       " 'review_body': 'Armband ist leider nach 1 Jahr kaputt gegangen',\n",
       " 'review_id': 'de_0203609',\n",
       " 'stars': 1,\n",
       " 'reviewer_id': 'reviewer_de_0267719',\n",
       " 'product_id': 'product_de_0865382',\n",
       " 'review_title': 'Leider nach 1 Jahr kaputt',\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dataset = product_dataset.map(map_labels)\n",
    "# Peek at the first example\n",
    "product_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43aa90f-71df-4c46-b724-42f1643e2bdc",
   "metadata": {},
   "source": [
    "Great, it works! We'll also need the reverse label mapping later, so let's define it here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d7b295a-32fb-461a-a246-bf68fe4da147",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d41cad-1a3c-4f7f-86df-8327aff415ea",
   "metadata": {},
   "source": [
    "## Creating a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25819eda-e329-4a9e-be0e-51cc6d43cb9a",
   "metadata": {},
   "source": [
    "A good practice when you're doing any sort of machine learning is to create a baseline model you can compare against. For text classification problems, a quick way to start is by using the zero-shot classification pipeline, which allows you to easily customise the number classes without having to train a model from scratch. \n",
    "\n",
    "Let's load one of the multilingual checkpoints from the Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0996437f-0004-49e8-aff7-0aac54824c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8496d763b9444d52951b8dab83ea0fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877651c54c7048668673e2f99f8365f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c87c61f6424c4b930b3c949c90a665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6324845a06d4074afc1b6b1505336e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d995550c6f12406c8173f8f522ab2429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "# `device` >= 0 places the model on the GPU\n",
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"vicgalle/xlm-roberta-large-xnli-anli\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780be01-26ee-43c9-a13f-361c2607d923",
   "metadata": {},
   "source": [
    "With this pipeline, we simply need to pass some text and the label names in the `candidate_labels` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4ecf890-4c7a-4aac-9355-4cd8af1c997c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Dieser TennisschlÃ¤ger ist perfekt!',\n",
       " 'labels': ['great', 'good', 'ok', 'poor', 'terrible'],\n",
       " 'scores': [0.41970810294151306,\n",
       "  0.38007867336273193,\n",
       "  0.19984789192676544,\n",
       "  0.00018881850701291114,\n",
       "  0.00017650889640208334]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_classifier(\"Dieser TennisschlÃ¤ger ist perfekt!\", candidate_labels=label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5b755-9d71-4ca9-9008-69aafdb45757",
   "metadata": {},
   "source": [
    "Now that we have our zero-shot pipeline, we'll use `Dataset.map()` to apply it to each example in the validation set. For that we'll need a small function to create a new column of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a3718b4-bad5-4e84-a187-89789e1d5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zeroshot_preds(examples):\n",
    "    preds = zeroshot_classifier(examples[\"review_body\"], candidate_labels=label_names)\n",
    "    label_pred = label2id[preds[\"labels\"][0]]\n",
    "    return {\"zeroshot_prediction\": label_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66bf5aaa-10c8-49d6-8dc2-b57f169cdd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9498116bb87640499504c33dfd6093db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/miniconda3/envs/hf/lib/python3.9/site-packages/transformers/pipelines/base.py:899: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "zeroshot_preds = product_dataset[\"validation\"].map(compute_zeroshot_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7a1d9c-eaf3-47e6-bcd0-20cafe93f6a7",
   "metadata": {},
   "source": [
    "Now that we've got some predictions, it's time to evaluate them! In the [MARC paper](https://arxiv.org/pdf/2010.02573.pdf), the authors point out that one should use the mean absolute error (MAE) for star ratings because:\n",
    "\n",
    "> star ratings for each review are ordinal, and a 2-star prediction for a 5-star review should be penalized more heavily than a 4-star prediction for a 5-star review.\n",
    "\n",
    "We'll take the same approach here and we can get the metric easily from Scikit-learn as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a740dc36-15c1-4880-ae31-61457499e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(zeroshot_preds[\"labels\"], zeroshot_preds[\"zeroshot_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8285d3-6812-4f80-97b1-14da65a80169",
   "metadata": {},
   "source": [
    "For reference, the MARC paper quotes MAE results from mBERT in the range of 0.5-0.7. Let's see if we can get close to that with XLM-RoBERTa!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67116204-d608-4c24-aa72-9f8d169987f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## From text to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44508e6-3d32-473e-8206-061ebb4c24d2",
   "metadata": {},
   "source": [
    "Like other machine learning models, Transformers expect their inputs in the form of numbers (not strings) and so some form of preprocessing is required. For NLP, this preprocessing step is called _tokenization_. Tokenization converts strings into atomic chunks called tokens, and these tokens are subsequently encoded as numerical vectors. \n",
    "\n",
    "Each pretrained model comes with its own tokenizer, so to get started let's download the tokenizer of XLM-RoBERTa from the Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2de857a8-ac62-4f0b-8376-1194718014a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeec647-d755-4ee0-bf77-2b746533c130",
   "metadata": {},
   "source": [
    "The tokenizer has a few interesting attributes such as the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42dde5dd-2d61-49c5-988a-c29aa547bafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250002"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad877e-7376-4103-ad56-a802d2ebad49",
   "metadata": {},
   "source": [
    "This tells us that XLM-R has 250,002 tokens that is can use to represent text with. Some of the tokens are called _special tokens_ to indicate whether a token is the start or end of a sentence, or corresponds to the mask that is associated with language modeling. Here's what the special tokens look like for XLM-R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a300b0c4-ee11-4bac-a398-ecfce41a504a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ffa45-cafa-4818-9dd8-4706e106570f",
   "metadata": {},
   "source": [
    "When you feed strings to the tokenizer, you'll get at least two fields (some models have more, depending on how they're trained):\n",
    "\n",
    "* `input_ids`: These correspond to the numerical encodings that map each token to an integer\n",
    "* `attention_mask`: This indicates to the model which tokens should be ignored when computing self-attention\n",
    "\n",
    "Let's see how this works with a simple example. First we encode the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e6080e6-4fd2-42b2-8c82-44cf38a43537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 37410, 33, 4837, 4, 8172, 15757, 443, 77330, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_str = tokenizer(\"Guten Tag, mein Name ist Lewis\")\n",
    "encoded_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa524f-afce-471a-8a5d-71a316b1160e",
   "metadata": {},
   "source": [
    "and then decode the input IDs to see the mapping explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9b53016-0c69-424f-8179-0608bac2df2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <s>\n",
      "37410 Gut\n",
      "33 en\n",
      "4837 Tag\n",
      "4 ,\n",
      "8172 mein\n",
      "15757 Name\n",
      "443 ist\n",
      "77330 Lewis\n",
      "2 </s>\n"
     ]
    }
   ],
   "source": [
    "for token in encoded_str[\"input_ids\"]:\n",
    "    print(token, tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277762a-1fc9-4919-8eca-91ae4cb5d3d9",
   "metadata": {},
   "source": [
    "So to prepare our inputs, we simply need to apply the tokenizer to each example in our corpus. As before, we'll do this with `Dataset.map()` so let's write a simple function to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efffa20f-2983-4e86-afa3-9bf8a8d44aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(examples):\n",
    "    return tokenizer(examples[\"review_body\"], truncation=True, max_length=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624a779-158e-4fc4-a8df-20e72a951bc3",
   "metadata": {},
   "source": [
    "Here we've enabled truncation, so the tokenizer will cut any inputs that are longer than 180 tokens (which is the setting used in the MARC paper). With this function we can go ahead and tokenize the whole corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0a246bc-9df9-4f80-9d68-b45a50c4ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-7c76818905c6886a.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-2a7fafc5fe9ed8ec.arrow\n",
      "Loading cached processed dataset at /data/.cache/hf/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-31f85c0c0b3e473c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label_name', 'labels', 'language', 'product_category', 'product_id', 'review_body', 'review_id', 'review_title', 'reviewer_id', 'stars'],\n",
       "        num_rows: 13748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label_name', 'labels', 'language', 'product_category', 'product_id', 'review_body', 'review_id', 'review_title', 'reviewer_id', 'stars'],\n",
       "        num_rows: 339\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label_name', 'labels', 'language', 'product_category', 'product_id', 'review_body', 'review_id', 'review_title', 'reviewer_id', 'stars'],\n",
       "        num_rows: 329\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = product_dataset.map(tokenize_reviews, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08caca6f-ec76-4d8a-9e4d-5d4140dbcd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'de',\n",
       " 'label_name': 'terrible',\n",
       " 'input_ids': [0,\n",
       "  33119,\n",
       "  8262,\n",
       "  443,\n",
       "  35103,\n",
       "  1561,\n",
       "  106,\n",
       "  7418,\n",
       "  42567,\n",
       "  3062,\n",
       "  6,\n",
       "  79659,\n",
       "  2],\n",
       " 'product_category': 'sports',\n",
       " 'review_body': 'Armband ist leider nach 1 Jahr kaputt gegangen',\n",
       " 'review_id': 'de_0203609',\n",
       " 'stars': 1,\n",
       " 'reviewer_id': 'reviewer_de_0267719',\n",
       " 'product_id': 'product_de_0865382',\n",
       " 'review_title': 'Leider nach 1 Jahr kaputt',\n",
       " 'labels': 0,\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19eace3-d188-4290-8c80-8640cf765ac5",
   "metadata": {},
   "source": [
    "This looks good, so now let's load a pretrained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0ef8f-95e8-4574-9a6e-4c5d9e638ca2",
   "metadata": {},
   "source": [
    "## Loading a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf7d53-e59d-4653-b589-f3939c456b27",
   "metadata": {},
   "source": [
    "To load a pretrained model from the Hub is quite simple: just select the appropriate `AutoModelForXxx` class and use the `from_pretrained()` function with the model checkpoint. In our case, we're dealing with 5 classes (one for each star) so to initialise the model we'll provide this information along with the label mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9765ff12-5395-446e-ac5a-6929e12098ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /data/.cache/hf/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"terrible\",\n",
      "    \"1\": \"poor\",\n",
      "    \"2\": \"ok\",\n",
      "    \"3\": \"good\",\n",
      "    \"4\": \"great\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"good\": 3,\n",
      "    \"great\": 4,\n",
      "    \"ok\": 2,\n",
      "    \"poor\": 1,\n",
      "    \"terrible\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /data/.cache/hf/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels, label2id=label2id, id2label=id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5f926-ee3b-40e1-9fb9-c5f415c1f909",
   "metadata": {},
   "source": [
    "These warnings are perfectly normal - they are telling us that the weights in the head of the network are randomly initialised and so we should fine-tune the model on a downstream task.\n",
    "\n",
    "Now that we have a model, the next step is to initialise a `Trainer` that will take care of the training loop for us. Let's do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c48c2-e7ee-4e0f-a31c-70a23c800530",
   "metadata": {},
   "source": [
    "## Creating a Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c9d9b-bf94-4a88-837b-3bdfd1b43f4e",
   "metadata": {},
   "source": [
    "To create a `Trainer`, we usually need a few basic ingredients:\n",
    "\n",
    "* A `TrainingArguments` class to define all the hyperparameters\n",
    "* A `compute_metrics` function to compute metrics during evaluation\n",
    "* Datasets to train and evaluate on\n",
    "\n",
    "Let's start with the `TrainingArguments`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0cddcb1-6695-4537-ab7d-be4798458b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b2d9f66-525e-4960-9860-1a8e7fb6f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "num_train_epochs = 2\n",
    "logging_steps = len(tokenized_dataset[\"train\"]) // (batch_size * num_train_epochs)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-marc\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93dd9e-0634-4c42-a9d1-096565312de7",
   "metadata": {},
   "source": [
    "Here we've defined `output_dir` to save our checkpoints and tweaked some of the default hyperparameters like the learning rate and weight decay. The `push_to_hub` argument will push each checkpoint to the Hub automatically for us, so we can reuse the model at any point in the future!\n",
    "\n",
    "Now that we've defined the hyperparameters, the next step is to define the metrics. As we did for the zero-shot pipeline, we'll use the mean absolute error, so we just wrap this logic in a simple function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f225d25-c669-4e3d-94f3-47b2ea61fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b605f30b-9fba-48dd-8d26-b96c09d1004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"MAE\": mean_absolute_error(labels, predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e8203-3b83-41f7-a6bc-2dfd11973b1c",
   "metadata": {},
   "source": [
    "With these ingredients we can now instantiate a `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d45d568c-c230-41c2-b73b-a1da577c9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/git/workshops/nlp-zurich/xlm-roberta-base-finetuned-marc is already a clone of https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer \n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08cb10-7c78-41d8-b8ba-815726192513",
   "metadata": {},
   "source": [
    "Note that here we've also provided the tokenizer to the `Trainer`: doing so will ensure that all of our examples are automatically padded to the longest example in each batch. This is needed so that matrix operations in the forward pass of the model can be computed. \n",
    "\n",
    "With our `Trainer`, it is then a simple matter to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6b1f0d7-e1a1-4658-813a-18f25d2aea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: label_name, language, product_category, review_body, review_id, stars, reviewer_id, product_id, review_title.\n",
      "***** Running training *****\n",
      "  Num examples = 13748\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1720' max='1720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1720/1720 06:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.033000</td>\n",
       "      <td>0.985195</td>\n",
       "      <td>0.507375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.905700</td>\n",
       "      <td>0.985361</td>\n",
       "      <td>0.480826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: label_name, language, product_category, review_body, review_id, stars, reviewer_id, product_id, review_title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 339\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc/checkpoint-860\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc/checkpoint-860/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc/checkpoint-860/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc/checkpoint-860/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc/checkpoint-860/special_tokens_map.json\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: label_name, language, product_category, review_body, review_id, stars, reviewer_id, product_id, review_title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 339\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc/checkpoint-1720\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc/checkpoint-1720/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc/checkpoint-1720/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc/checkpoint-1720/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc/checkpoint-1720/special_tokens_map.json\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1720, training_loss=1.0193121623161228, metrics={'train_runtime': 412.588, 'train_samples_per_second': 66.643, 'train_steps_per_second': 4.169, 'total_flos': 1969525631983536.0, 'train_loss': 1.0193121623161228, 'epoch': 2.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c701a-b01f-489a-bf20-9ded4b6cbaf2",
   "metadata": {},
   "source": [
    "Nice, with just a few mintues of training, we've managed to halve our error compared to the zero-shot baseline! After training is complete, we can push the commits to our repository on the Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9dae8-894c-4427-b2fd-8ac4a45d168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to xlm-roberta-base-finetuned-marc\n",
      "Configuration saved in xlm-roberta-base-finetuned-marc/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-marc/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-marc/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-marc/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ab12752cbd447b8f7959e6be1fd62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e096f0fdaf4044a1aa16e3aa5a5569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Oct15_19-59-11_vorace/events.out.tfevents.1634320897.vorace: 100%|##########| 4.93k/4.93k [00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc\n",
      "   7308cc1..5b879d2  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary field:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'dataset': {'name': 'amazon_reviews_multi', 'type': 'amazon_reviews_multi', 'args': 'de'}}\n",
      "To https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc\n",
      "   5b879d2..a6ab0d4  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/commit/5b879d262ec86ee4cbfd23f40f733ae5edb9c7f6'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59472032-1c4a-400c-9eb4-324c40515c0c",
   "metadata": {},
   "source": [
    "## Evaluating cross-lingual transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5d652-5d56-4bae-a444-03c8a7f1e584",
   "metadata": {},
   "source": [
    "Now that we're fine-tuned our model on a German subset, we can evaluate its ability to transfer to other languages. To do so, we'll load the validation set in a given language, apply the same filtering and preprocessing that we did for the German subset, and finally use `Trainer.evaluate()` to compute the metrics. The following function does the trick: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77c44999-5a1d-4d34-8a72-81936158bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_corpus(lang):\n",
    "    # Load the language subset\n",
    "    dataset = load_dataset(dataset_name, lang, split=\"validation\")\n",
    "    # Filter for the `sports` product category\n",
    "    product_dataset = dataset.filter(filter_for_product)\n",
    "    # Map and create label columns\n",
    "    product_dataset = product_dataset.map(map_labels)\n",
    "    # Tokenize the inputs\n",
    "    tokenized_dataset = product_dataset.map(tokenize_reviews, batched=True)\n",
    "    # Generate predictions and metrics\n",
    "    preds = trainer.evaluate(eval_dataset=tokenized_dataset)\n",
    "    return {\"MAE\": preds[\"eval_MAE\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e43e7c-98b6-43b9-83cb-58218b51a45f",
   "metadata": {},
   "source": [
    "Let's start with English (for reference our MAE on German was around 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "172ae1fb-e5a3-437d-bfc1-752837b010fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f459fafef17c42d58acc932a0168c811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db75de6ef0841358ac7ba152728dbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc31a89847c4ec7ab7b8e53fcde06df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: label_name, language, product_category, review_body, review_id, stars, reviewer_id, product_id, review_title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 225\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.52}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_corpus(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f2bc2-3d74-40f6-b5cb-d1903f0fbbb8",
   "metadata": {},
   "source": [
    "Not bad! Our fine-tuned German model is able to transfer to English at roughly the same performance. How about French?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4be0db14-2abc-4597-ade1-a27196a95e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (/data/.cache/hf/datasets/amazon_reviews_multi/fr/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25e2825c9154958b171f4c974a8ba67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c233e2ed7dab4e2fb3a3beea0b957131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9b6e499de646849790a4f137eba7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: label_name, language, product_category, review_body, review_id, stars, reviewer_id, product_id, review_title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.5142857142857142}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_corpus(\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ff17c-aaf1-474a-8baa-4ac95d34f0fb",
   "metadata": {},
   "source": [
    "Nice, this is very similar too! This shows the great power of multilingual models - provided your target language was included in the pretraining, there's a good chance you'll only need to tune and deploy a single model in production instead of running one per language. \n",
    "\n",
    "This wraps up our training and evaluation step - one last thing to try is seeing how we can interact with our model in a `pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af782a3-6b3c-4597-8490-d9ca5ce37933",
   "metadata": {},
   "source": [
    "## Using your fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50100e5f-2344-4baf-9a9d-acf514693651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/config.json not found in cache or force_download set to True, downloading to /data/.cache/hf/transformers/tmpk7o8hszw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b27df8706eb4bed91ff771e52d8c38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/976 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/config.json in cache at /data/.cache/hf/transformers/708cb2dccaf4e0ade4961fd84072ddc367ccd3f880e82da2039b2f8b66d3364e.2b84791fe464e01eeffac0776c6630b7f5c8a4aa14a39ee1292d706acb9346f2\n",
      "creating metadata file for /data/.cache/hf/transformers/708cb2dccaf4e0ade4961fd84072ddc367ccd3f880e82da2039b2f8b66d3364e.2b84791fe464e01eeffac0776c6630b7f5c8a4aa14a39ee1292d706acb9346f2\n",
      "loading configuration file https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/config.json from cache at /data/.cache/hf/transformers/708cb2dccaf4e0ade4961fd84072ddc367ccd3f880e82da2039b2f8b66d3364e.2b84791fe464e01eeffac0776c6630b7f5c8a4aa14a39ee1292d706acb9346f2\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"terrible\",\n",
      "    \"1\": \"poor\",\n",
      "    \"2\": \"ok\",\n",
      "    \"3\": \"good\",\n",
      "    \"4\": \"great\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"good\": 3,\n",
      "    \"great\": 4,\n",
      "    \"ok\": 2,\n",
      "    \"poor\": 1,\n",
      "    \"terrible\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/config.json from cache at /data/.cache/hf/transformers/708cb2dccaf4e0ade4961fd84072ddc367ccd3f880e82da2039b2f8b66d3364e.2b84791fe464e01eeffac0776c6630b7f5c8a4aa14a39ee1292d706acb9346f2\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"terrible\",\n",
      "    \"1\": \"poor\",\n",
      "    \"2\": \"ok\",\n",
      "    \"3\": \"good\",\n",
      "    \"4\": \"great\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"good\": 3,\n",
      "    \"great\": 4,\n",
      "    \"ok\": 2,\n",
      "    \"poor\": 1,\n",
      "    \"terrible\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /data/.cache/hf/transformers/tmp51yc7zr1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065d924d8c044a5188fee9d24922edc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/pytorch_model.bin in cache at /data/.cache/hf/transformers/518a14fcd68b41e35e993f5db4923f9e07232e05205705d44830e769d2b21c2a.44a35cbe569be032eda61c76242ca7986aba02556a895ba6d8b1872bd3f32e6e\n",
      "creating metadata file for /data/.cache/hf/transformers/518a14fcd68b41e35e993f5db4923f9e07232e05205705d44830e769d2b21c2a.44a35cbe569be032eda61c76242ca7986aba02556a895ba6d8b1872bd3f32e6e\n",
      "loading weights file https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/pytorch_model.bin from cache at /data/.cache/hf/transformers/518a14fcd68b41e35e993f5db4923f9e07232e05205705d44830e769d2b21c2a.44a35cbe569be032eda61c76242ca7986aba02556a895ba6d8b1872bd3f32e6e\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at lewtun/xlm-roberta-base-finetuned-marc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /data/.cache/hf/transformers/tmpnbwoaiyz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92504d107076410ba25c5ebab4c12a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/398 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/tokenizer_config.json in cache at /data/.cache/hf/transformers/2f435776d6304cbf50eaead7e502081f62e039b6e109d72563f9d8c078dcc24c.b36482fbec4a714d3cfec99e0b05f4fdeec9e759090a78aed5597583a8b4783d\n",
      "creating metadata file for /data/.cache/hf/transformers/2f435776d6304cbf50eaead7e502081f62e039b6e109d72563f9d8c078dcc24c.b36482fbec4a714d3cfec99e0b05f4fdeec9e759090a78aed5597583a8b4783d\n",
      "https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /data/.cache/hf/transformers/tmp2dlc3brm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a57a5bb43054076a92e7831d615be20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/sentencepiece.bpe.model in cache at /data/.cache/hf/transformers/333fe27ab4d1569387cf119df3317f951e9033c50c47a7a8aae423a3db516bca.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n",
      "creating metadata file for /data/.cache/hf/transformers/333fe27ab4d1569387cf119df3317f951e9033c50c47a7a8aae423a3db516bca.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n",
      "https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /data/.cache/hf/transformers/tmphyz1pw99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c109d6f3ea43cfa5a000174dece298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/tokenizer.json in cache at /data/.cache/hf/transformers/b426b5da2fb3b3de00fd86ff9ca65b0a30c01b5ae21340781e9105feac50baec.efb6ce620134b7ce68e8535339704894eb0fd7c4001a54ec8a6e7f6bd7bfd836\n",
      "creating metadata file for /data/.cache/hf/transformers/b426b5da2fb3b3de00fd86ff9ca65b0a30c01b5ae21340781e9105feac50baec.efb6ce620134b7ce68e8535339704894eb0fd7c4001a54ec8a6e7f6bd7bfd836\n",
      "https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /data/.cache/hf/transformers/tmprz1ku1wj\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4809f28daad5430cb7498fd0ec381c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/special_tokens_map.json in cache at /data/.cache/hf/transformers/2e5241b31b3667f90c2cfebabc29f3d217f6c849cd02f2b25aff605db2ad81f2.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "creating metadata file for /data/.cache/hf/transformers/2e5241b31b3667f90c2cfebabc29f3d217f6c849cd02f2b25aff605db2ad81f2.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/sentencepiece.bpe.model from cache at /data/.cache/hf/transformers/333fe27ab4d1569387cf119df3317f951e9033c50c47a7a8aae423a3db516bca.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n",
      "loading file https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/tokenizer.json from cache at /data/.cache/hf/transformers/b426b5da2fb3b3de00fd86ff9ca65b0a30c01b5ae21340781e9105feac50baec.efb6ce620134b7ce68e8535339704894eb0fd7c4001a54ec8a6e7f6bd7bfd836\n",
      "loading file https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/special_tokens_map.json from cache at /data/.cache/hf/transformers/2e5241b31b3667f90c2cfebabc29f3d217f6c849cd02f2b25aff605db2ad81f2.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/lewtun/xlm-roberta-base-finetuned-marc/resolve/main/tokenizer_config.json from cache at /data/.cache/hf/transformers/2f435776d6304cbf50eaead7e502081f62e039b6e109d72563f9d8c078dcc24c.b36482fbec4a714d3cfec99e0b05f4fdeec9e759090a78aed5597583a8b4783d\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "finetuned_checkpoint = \"lewtun/xlm-roberta-base-finetuned-marc\"\n",
    "classifier = pipeline(\"text-classification\", model=finetuned_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2582a5b7-0fc1-4b48-9e10-8a8b4ac76af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'great', 'score': 0.8492617011070251}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I love these running shoes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428f746-d2c2-43b3-ac9d-d5b3a71baf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
